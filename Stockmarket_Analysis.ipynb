{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lean-IQ/Stockmarket-Analysis/blob/main/Stockmarket_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock Market Analysis"
      ],
      "metadata": {
        "id": "ygBQnms5I1Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About**\n",
        "\n",
        "This Python script is designed for analyzing stock market performance using a selection of technical indicators. The primary goals of this script are:\n",
        "\n",
        "1.   Trend Analysis: Identify and analyze market trends using indicators such as Moving Averages (MA), Relative Strength Index (RSI), and Moving Average Convergence Divergence (MACD).\n",
        "\n",
        "2.   Signal Generation: Generate buy and sell signals based on the computed indicators, helping to inform trading decisions.\n",
        "\n",
        "3.   Performance Evaluation: Assess the performance of selected stocks over time, providing insights into potential investment opportunities.\n",
        "\n",
        "4.   Visualization: Provide intuitive visual representations of stock data and indicators, aiding in the interpretation of market trends and signals.\n",
        "\n",
        "5. Data-Driven Insights: Utilize historical stock data to backtest strategies and validate the effectiveness of selected indicators.\n",
        "\n",
        "6. Automation: Automate the process of data retrieval, analysis, and visualization to ensure efficiency and accuracy.\n",
        "\n",
        "7. Customization: Allow users to customize the parameters of technical indicators to tailor the analysis to specific trading strategies or preferences.\n",
        "\n",
        "This script leverages Python libraries such as pandas for data manipulation, matplotlib for plotting, and yfinance for retrieving stock data, ensuring a comprehensive and efficient analysis toolkit for market participants."
      ],
      "metadata": {
        "id": "EFh2abYhJY45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SBxo3AfYJ671"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**License**\n",
        "\n",
        "This project is licensed under the MIT License. You are free to use, modify, and distribute this software in accordance with the terms specified in the MIT License. A copy of the license is included in the project repository."
      ],
      "metadata": {
        "id": "ou0YJObUJ9uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Required Libraries"
      ],
      "metadata": {
        "id": "TTspzIZmP6vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzHX_oGXJZvr"
      },
      "outputs": [],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raw Data Picking\n",
        "\n",
        "**Usage**\n",
        "\n",
        "* Dependencies: Ensure you have the required Python libraries installed, including yfinance, pandas, and google.colab (if running in Google Colab).\n",
        "\n",
        "* Google Drive Setup: If running in Google Colab, mount your Google Drive to access files using the appropriate commands.\n",
        "\n",
        "* File Paths: Define the paths to your CSV file containing ticker symbols and the directory where raw data will be saved. Modify the csv_file_path and raw_data_path variables as needed to match your directory structure.\n",
        "\n",
        "* Running the Script: Execute the script to fetch and save data for each ticker symbol in your portfolio. Load the CSV file containing ticker symbols and iterate over each symbol to fetch and save historical data.\n",
        "\n",
        "**Script Explanation**\n",
        "\n",
        "The script performs the following key tasks:\n",
        "\n",
        "* Mount Google Drive: Mounts Google Drive to access the necessary files if running in Google Colab.\n",
        "\n",
        "* Define File Paths: Specifies the paths to the CSV file with ticker symbols and the directory for saving raw data.\n",
        "\n",
        "* Check File Age: Contains a function to check if a file exists and is older than one day, ensuring data is up-to-date.\n",
        "\n",
        "* Fetch and Save Data: Defines a function to fetch historical stock data for a given ticker symbol using the yfinance library, and saves the data to a CSV file with specific formatting. The function also handles errors and prints relevant messages.\n",
        "\n",
        "* Load Ticker Symbols: Loads ticker symbols from the specified CSV file into a DataFrame.\n",
        "\n",
        "* Iterate and Fetch Data: Iterates over the ticker symbols in the DataFrame, calling the function to fetch and save data for each symbol."
      ],
      "metadata": {
        "id": "ZklyPZWVKmBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVmEFhtjQfDL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# PICK RAW FINANCIAL AND STOCK DATA FOR ALL PORTFOLIO COMPANIES\n",
        "\n",
        "import os\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Mount Google Drive if necessary\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths\n",
        "csv_file_path = '/content/drive/MyDrive/DATA/portfolio.csv'\n",
        "raw_data_path = '/content/drive/MyDrive/DATA/RAW/'\n",
        "\n",
        "# Function to check if a file exists and is older than 1 day\n",
        "def is_file_older_than_1_day(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        return False\n",
        "    modification_time = os.path.getmtime(file_path)\n",
        "    time_difference = datetime.now() - datetime.fromtimestamp(modification_time)\n",
        "    return time_difference.days > 1\n",
        "\n",
        "# Function to fetch historical data for a ticker symbol and save it to CSV\n",
        "def fetch_and_save_data(ticker):\n",
        "    try:\n",
        "        # Check if CSV file already exists and is older than 1 day\n",
        "        ticker_csv_path = os.path.join(raw_data_path, f'{ticker}.csv')\n",
        "        if is_file_older_than_1_day(ticker_csv_path):\n",
        "            os.remove(ticker_csv_path)  # Remove the file if it's older than 1 day\n",
        "\n",
        "        # Fetch last trading day\n",
        "        last_trading_day = yf.download(\"SPY\").index[-1]\n",
        "\n",
        "        # Calculate start date 1 year back from the last trading day\n",
        "        start_date = last_trading_day - timedelta(days=365)\n",
        "\n",
        "        # Fetch historical data\n",
        "        ticker_data = yf.download(ticker, start=start_date, end=last_trading_day)\n",
        "\n",
        "        # Save data to CSV with specific formatting\n",
        "        ticker_data.to_csv(ticker_csv_path, float_format='%.5f')  # Control decimal precision\n",
        "        print(f\"Data for {ticker} saved to {ticker_csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "\n",
        "# Load CSV file containing ticker symbols\n",
        "portfolio_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Iterate over ticker symbols and fetch data\n",
        "for ticker in portfolio_df['YFINANCE']:\n",
        "    fetch_and_save_data(ticker)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Stochastic Indicator\n",
        "\n",
        "**Usage**\n",
        "\n",
        "- Dependencies: Ensure you have the required Python libraries installed, including pandas and numpy.\n",
        "\n",
        "- File Paths: Define the paths for the raw data directory and the output directory. Modify the raw_data_path and output_path variables to match your directory structure.\n",
        "\n",
        "- Data Preparation: Ensure your raw data CSV files are placed in the specified raw data directory. The script expects the files to have columns such as 'Low', 'High', and 'Close'.\n",
        "\n",
        "- Running the Script: Execute the script to analyze the Stochastic indicator for each CSV file in the raw data directory and save the results to the output directory.\n",
        "\n",
        "**Script Explanation**\n",
        "\n",
        "The script performs the following key tasks:\n",
        "\n",
        "- File Paths: Specifies the paths for the raw data directory and the directory where the analyzed data will be saved.\n",
        "\n",
        "- Stochastic Calculation: Defines a function to calculate the Stochastic indicator, including the %K and %D lines, based on given periods.\n",
        "\n",
        "- Stochastic Analysis: Defines a function to analyze the Stochastic indicator for various trading signals and divergences. The function identifies trends such as bullish and bearish divergences, crossover signals, and potential buy/sell signals.\n",
        "\n",
        "- Analyze Single CSV: Contains a function to load a single CSV file, calculate the Exponential Moving Average (EMA200), apply the Stochastic calculation and analysis, and save the results to the output directory. The function ensures the output is saved with appropriate formatting and decimal separators.\n",
        "\n",
        "- Analyze All CSV Files: Defines a function to iterate over all CSV files in the raw data directory and apply the analysis function to each file. It collects the list of CSV files, processes each file, and prints a completion message once all files are analyzed.\n",
        "\n",
        "- Execution: Calls the function to analyze all CSV files in the raw data directory, triggering the Stochastic calculation and analysis for each file, and saving the results to the specified output directory."
      ],
      "metadata": {
        "id": "sYYTKZJpLWoJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIZAnN3Wus1t"
      },
      "outputs": [],
      "source": [
        "# 02STOCH\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define file paths\n",
        "raw_data_path = '/content/drive/MyDrive/DATA/RAW'\n",
        "output_path = '/content/drive/MyDrive/DATA/STOCHASTIC'\n",
        "\n",
        "# Function to calculate Stochastic indicator\n",
        "def calculate_stochastic(df, k_period=14, d_period=3):\n",
        "    # Calculate %K\n",
        "    lowest_low = df['Low'].rolling(window=k_period).min()\n",
        "    highest_high = df['High'].rolling(window=k_period).max()\n",
        "    df['%K'] = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)\n",
        "    # Calculate %D\n",
        "    df['%D'] = df['%K'].rolling(window=d_period).mean()\n",
        "    return df\n",
        "\n",
        "# Function to analyze Stochastic indicator\n",
        "def analyze_stochastic(df):\n",
        "    # Trending Bullish Divergence\n",
        "    df['Trending Bullish Divergence'] = np.where((df['Close'] > df['EMA200']) & (df['%K'] < df['%D']), 'Yes', 'No')\n",
        "    # Regular Bullish Divergence\n",
        "    df['Regular Bullish Divergence'] = np.where((df['Close'].shift(1) < df['Close']) & (df['%K'].shift(1) > df['%K']) & (df['%D'].shift(1) < df['%D']), 'Yes', 'No')\n",
        "    # Hidden Bullish Divergence\n",
        "    df['Hidden Bullish Divergence'] = np.where((df['Close'].shift(1) > df['Close']) & (df['%K'].shift(1) < df['%K']) & (df['%D'].shift(1) > df['%D']), 'Yes', 'No')\n",
        "    # Trending Bearish Divergence\n",
        "    df['Trending Bearish Divergence'] = np.where((df['Close'] < df['EMA200']) & (df['%K'] > df['%D']), 'Yes', 'No')\n",
        "    # Regular Bearish Divergence\n",
        "    df['Regular Bearish Divergence'] = np.where((df['Close'].shift(1) > df['Close']) & (df['%K'].shift(1) < df['%K']) & (df['%D'].shift(1) > df['%D']), 'Yes', 'No')\n",
        "    # Hidden Bearish Divergence\n",
        "    df['Hidden Bearish Divergence'] = np.where((df['Close'].shift(1) < df['Close']) & (df['%K'].shift(1) > df['%K']) & (df['%D'].shift(1) < df['%D']), 'Yes', 'No')\n",
        "    # Middle Filter Aktive\n",
        "    df['Middle Filter Aktive'] = np.where((df['%K'] > 50) & (df['%D'] > 50), 'Yes', 'No')\n",
        "    # Short Signal\n",
        "    df['Short Signal'] = np.where((df['Close'] < df['EMA200']) & (df['%K'] > 80) & (df['%K'].shift(1) > 80), 'Yes', 'No')\n",
        "    # Long Signal\n",
        "    df['Long Signal'] = np.where((df['Close'] > df['EMA200']) & (df['%K'] < 20) & (df['%K'].shift(1) < 20), 'Yes', 'No')\n",
        "    # Crossing\n",
        "    df['Sell Signal'] = np.where((df['%K'] < df['%D']) & (df['%K'].shift(1) > df['%D'].shift(1)), 'Yes', 'No')\n",
        "    df['Sell Signal, Extreme'] = np.where((df['%K'] < 80) & (df['%D'] < 80) & (df['%K'].shift(1) > 80) & (df['%D'].shift(1) > 80), 'Yes', 'No')\n",
        "    df['Buy Signal'] = np.where((df['%K'] > df['%D']) & (df['%K'].shift(1) < df['%D'].shift(1)), 'Yes', 'No')\n",
        "    df['Buy Signal, Extreme'] = np.where((df['%K'] > 20) & (df['%D'] > 20) & (df['%K'].shift(1) < 20) & (df['%D'].shift(1) < 20), 'Yes', 'No')\n",
        "    # Crossover\n",
        "    df['Right Hand Crossover'] = np.where((df['%D'].shift(1) > df['%K'].shift(1)) & (df['%D'] < df['%K']), 'Yes', 'No')\n",
        "    df['Left Hand Crossover'] = np.where((df['%K'].shift(1) > df['%D'].shift(1)) & (df['%K'] < df['%D']), 'Yes', 'No')\n",
        "    return df\n",
        "\n",
        "# Function to analyze Stochastic for a single CSV file\n",
        "def analyze_single_csv(file_path):\n",
        "    try:\n",
        "        # Load data\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Calculate EMA200\n",
        "        df['EMA200'] = df['Close'].ewm(span=200, min_periods=0).mean()\n",
        "        # Calculate Stochastic indicator\n",
        "        df = calculate_stochastic(df)\n",
        "        # Analyze Stochastic indicator\n",
        "        df = analyze_stochastic(df)\n",
        "        # Output results\n",
        "        filename = os.path.basename(file_path)\n",
        "        output_file_path = os.path.join(output_path, filename)\n",
        "        df.to_csv(output_file_path, index=False, mode='w', decimal=',')  # Ensure overwrite mode and use \",\" as decimal separator\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing Stochastic for {file_path}: {e}\")\n",
        "\n",
        "# Function to analyze Stochastic for all CSV files in the RAW folder\n",
        "def analyze_all_csv_files():\n",
        "    # Get list of CSV files\n",
        "    csv_files = [os.path.join(raw_data_path, file) for file in os.listdir(raw_data_path) if file.endswith('.csv')]\n",
        "    # Analyze Stochastic for each CSV file\n",
        "    for file_path in csv_files:\n",
        "        analyze_single_csv(file_path)\n",
        "    print(\"Stochastic analysis for all files completed.\")\n",
        "\n",
        "# Analyze Stochastic for all CSV files in the RAW folder\n",
        "analyze_all_csv_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastics Data Consolidation**"
      ],
      "metadata": {
        "id": "CTvi49z5Ly3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u9x1XHAfpIH"
      },
      "outputs": [],
      "source": [
        "# 03SUMSTO\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "stochastic_path = '/content/drive/MyDrive/DATA/STOCHASTIC'\n",
        "output_file_path = '/content/drive/MyDrive/DATA/results_stochastic.csv'\n",
        "\n",
        "# Function to read CSV files and extract rows with the latest date\n",
        "def extract_latest_data(file_path, file_name):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        latest_date = df['Date'].max()  # Find the latest date in the dataframe\n",
        "        latest_data = df[df['Date'] == latest_date].copy()  # Extract rows with the latest date and create a copy\n",
        "        latest_data['File'] = file_name  # Add a new column with the file name\n",
        "        return latest_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate through CSV files in the STOCHASTIC input path\n",
        "for file_name in os.listdir(stochastic_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(stochastic_path, file_name)\n",
        "        # Extract rows with the latest date from the current file\n",
        "        latest_data = extract_latest_data(file_path, file_name)\n",
        "        if latest_data is not None:\n",
        "            # Append the latest data to the list of dataframes\n",
        "            dfs.append(latest_data)\n",
        "\n",
        "# Check if any dataframes were extracted\n",
        "if dfs:\n",
        "    # Concatenate dataframes in the list into a single dataframe\n",
        "    consolidated_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Check if the output file already exists and remove it\n",
        "    if os.path.exists(output_file_path):\n",
        "        os.remove(output_file_path)\n",
        "\n",
        "    # Save the consolidated data to a CSV file\n",
        "    consolidated_data.to_csv(output_file_path, index=False, decimal=',')\n",
        "    print(\"Latest STOCHASTIC data with file names has been consolidated and saved to:\", output_file_path)\n",
        "else:\n",
        "    print(\"No STOCHASTIC data found in the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Relative Strength Indicator (RSI)\n",
        "\n",
        "**Usage**\n",
        "\n",
        "- Dependencies: Ensure you have the required Python libraries installed, including pandas and numpy.\n",
        "\n",
        "- File Paths: Define the paths for the raw data directory and the RSI output directory. Modify the raw_data_path and rsi_output_path variables to match your directory structure.\n",
        "\n",
        "- Data Preparation: Ensure your raw data CSV files are placed in the specified raw data directory. The script expects the files to have a 'Close' column for the calculation of RSI.\n",
        "\n",
        "- Running the Script: Execute the script to calculate and interpret the RSI for each CSV file in the raw data directory and save the results to the RSI output directory.\n",
        "\n",
        "**Script Explanation**\n",
        "\n",
        "The script performs the following key tasks:\n",
        "\n",
        "- File Paths: Specifies the paths for the raw data directory and the directory where the RSI data will be saved.\n",
        "\n",
        "- RSI Calculation: Defines a function to calculate the Relative Strength Index (RSI) over a given period. This function calculates the RSI based on the difference in closing prices over time, distinguishing between gains and losses.\n",
        "\n",
        "- RSI Signal Classification: Defines a function to classify RSI values into signals such as \"Overbought\", \"Oversold\", and \"Neutral\" based on standard RSI thresholds.\n",
        "\n",
        "- RSI Interpretation: Contains a function to interpret RSI signals, including:\n",
        "\n",
        "1. *Calculation of 14-day and 30-day RSI values.*\n",
        "\n",
        "2. *Classification of RSI signals.*\n",
        "\n",
        "3. *Trend interpretation based on RSI changes.*\n",
        "\n",
        "4. *Divergence interpretation using moving averages for detecting bullish and bearish divergences.*\n",
        "\n",
        "- Read and Process Data: Iterates over all CSV files in the raw data directory, reads each file into a DataFrame, and applies the RSI interpretation function. The interpreted data includes RSI values, signal classifications, trend directions, and divergence indicators.*\n",
        "\n",
        "- Save Results: Saves the interpreted RSI data for each processed CSV file to the specified RSI output directory, ensuring the results are stored with appropriate formatting.\n",
        "\n",
        "- Execution: After processing all CSV files, the script prints a completion message indicating that the RSI calculation and interpretation are finished and the results are saved."
      ],
      "metadata": {
        "id": "1YJwLmPwMD6Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjMGgUheyFXc"
      },
      "outputs": [],
      "source": [
        "# 04RSI\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "raw_data_path = '/content/drive/MyDrive/DATA/RAW'\n",
        "rsi_output_path = '/content/drive/MyDrive/DATA/RSI'\n",
        "\n",
        "# Function to calculate RSI\n",
        "def calculate_rsi(data, period):\n",
        "    delta = data.diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Function to classify RSI signals based on interpretation\n",
        "def classify_rsi(rsi):\n",
        "    signals = []\n",
        "    for val in rsi:\n",
        "        if val > 70:\n",
        "            signals.append(\"Overbought\")\n",
        "        elif val < 30:\n",
        "            signals.append(\"Oversold\")\n",
        "        else:\n",
        "            signals.append(\"Neutral\")\n",
        "    return signals\n",
        "\n",
        "# Function to interpret RSI signals based on trend and divergence criteria\n",
        "def interpret_rsi(data):\n",
        "    data['RSI_14'] = calculate_rsi(data['Close'], 14)\n",
        "    data['RSI_30'] = calculate_rsi(data['Close'], 30)\n",
        "    data['RSI_14_Class'] = classify_rsi(data['RSI_14'])\n",
        "    data['RSI_30_Class'] = classify_rsi(data['RSI_30'])\n",
        "    # Trend interpretation\n",
        "    data['Trend_14'] = np.where(data['RSI_14'].diff() > 0, \"Rising\", \"Falling\")\n",
        "    data['Trend_30'] = np.where(data['RSI_30'].diff() > 0, \"Rising\", \"Falling\")\n",
        "    # Divergence interpretation (for 14-day period)\n",
        "    data['RSI_14_MA'] = data['RSI_14'].rolling(window=14).mean()\n",
        "    data['Close_14_MA'] = data['Close'].rolling(window=14).mean()\n",
        "    data['Bullish_Divergence'] = np.where((data['RSI_14'] > data['RSI_14_MA'].shift(1)) & (data['Close'] < data['Close_14_MA'].shift(1)), \"Yes\", \"No\")\n",
        "    data['Bearish_Divergence'] = np.where((data['RSI_14'] < data['RSI_14_MA'].shift(1)) & (data['Close'] > data['Close_14_MA'].shift(1)), \"Yes\", \"No\")\n",
        "    return data  # Return the modified DataFrame\n",
        "\n",
        "# Read data from CSV files in the raw data path\n",
        "for file_name in os.listdir(raw_data_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(raw_data_path, file_name)\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Interpret RSI signals\n",
        "        df_interpreted = interpret_rsi(df)\n",
        "        # Save interpreted RSI data to CSV file in the output path\n",
        "        output_file_path = os.path.join(rsi_output_path, file_name)\n",
        "        df_interpreted.to_csv(output_file_path, index=False, decimal=',')\n",
        "\n",
        "print(\"RSI calculation and interpretation completed. Results saved to the RSI output path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSI Data Consolidation**"
      ],
      "metadata": {
        "id": "eBlSC7_gNAUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IUDPypbBwUn"
      },
      "outputs": [],
      "source": [
        "# 05SUMRSI\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "rsi_input_path = '/content/drive/MyDrive/DATA/RSI'\n",
        "output_file_path = '/content/drive/MyDrive/DATA/results_rsi.csv'\n",
        "\n",
        "# Function to read CSV files and extract rows with the latest date\n",
        "def extract_latest_data(file_path, file_name):\n",
        "    df = pd.read_csv(file_path)\n",
        "    latest_date = df['Date'].max()  # Find the latest date in the dataframe\n",
        "    latest_data = df[df['Date'] == latest_date].copy()  # Extract rows with the latest date and create a copy\n",
        "    latest_data['File'] = file_name  # Add a new column with the file name\n",
        "    return latest_data\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate through CSV files in the RSI input path\n",
        "for file_name in os.listdir(rsi_input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(rsi_input_path, file_name)\n",
        "        # Extract rows with the latest date from the current file\n",
        "        latest_data = extract_latest_data(file_path, file_name)\n",
        "        # Append the latest data to the list of dataframes\n",
        "        dfs.append(latest_data)\n",
        "\n",
        "# Concatenate dataframes in the list into a single dataframe\n",
        "consolidated_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Check if the output file already exists\n",
        "if os.path.exists(output_file_path):\n",
        "    # Remove the existing file\n",
        "    os.remove(output_file_path)\n",
        "\n",
        "# Save the consolidated data to a CSV file\n",
        "consolidated_data.to_csv(output_file_path, index=False, decimal=',')\n",
        "\n",
        "print(\"Latest RSI data with file names has been consolidated and saved to:\", output_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Moving average convergence and divergence (MACD) Indocator\n",
        "\n",
        "**Usage**\n",
        "- Dependencies: Ensure you have the required Python libraries installed, including pandas and numpy.\n",
        "\n",
        "- File Paths: Define the paths for the raw data directory and the MACD output directory. Modify the raw_data_path and macd_output_path variables to match your directory structure.\n",
        "\n",
        "- Data Preparation: Ensure your raw data CSV files are placed in the specified raw data directory. The script expects the files to have a 'Close' column for the calculation of MACD.\n",
        "\n",
        "- Running the Script: Execute the script to calculate and interpret the Moving Average Convergence Divergence (MACD) for each CSV file in the raw data directory and save the results to the MACD output directory.\n",
        "\n",
        "**Script Explanation**\n",
        "The script performs the following key tasks:\n",
        "\n",
        "- File Paths: Specifies the paths for the raw data directory and the directory where the MACD data will be saved.\n",
        "\n",
        "- MACD Calculation: Defines a function to calculate the MACD parameters, including:\n",
        "\n",
        "1. *MACD Line: The difference between the 12-day Exponential Moving Average (EMA) and the 26-day EMA.*\n",
        "2. *MACD Signal Line: The 9-day EMA of the MACD line.*\n",
        "3. *MACD Histogram: The difference between the MACD line and the MACD Signal line.*\n",
        "\n",
        "- MACD Signal Interpretation: Defines a function to interpret MACD signals, including:\n",
        "\n",
        "1. *Buy Signal: When the MACD crosses its Signal line from bottom to top.*\n",
        "2. *Sell Signal: When the MACD crosses its Signal line from top to bottom.*\n",
        "3. *Signal Intensity: Average distance of MACD and Signal line to the midpoint line.*\n",
        "4. *Upward Momentum: When the MACD is higher than its nine-day average.*\n",
        "5. *Downward Momentum: When the MACD is lower than its nine-day average.*\n",
        "6. *Open Bullish Position: When the MACD shows an uptrend over a period of 14 days.*\n",
        "\n",
        "- Open Bearish Position: When the MACD shows a downtrend over a period of 14 days.\n",
        "- Remove Existing Files: Removes any existing CSV files in the MACD output directory to ensure fresh data storage.\n",
        "\n",
        "- Read and Process Data: Iterates over all CSV files in the raw data directory, reads each file into a DataFrame, calculates the MACD parameters, interprets MACD signals, and saves the interpreted MACD data for each file to the specified MACD output directory.\n",
        "\n",
        "- Execution: After processing all CSV files, the script prints a completion message indicating that the MACD calculation and interpretation are finished and the results are saved."
      ],
      "metadata": {
        "id": "4JsprnXeNSkT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylmd9NaHkbjB"
      },
      "outputs": [],
      "source": [
        "# 06MACD\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "raw_data_path = '/content/drive/MyDrive/DATA/RAW'\n",
        "macd_output_path = '/content/drive/MyDrive/DATA/MACD'\n",
        "\n",
        "# Function to calculate MACD parameters\n",
        "def calculate_macd(data):\n",
        "    # Calculate MACD line (12-day EMA - 26-day EMA)\n",
        "    data['EMA_12'] = data['Close'].ewm(span=12, min_periods=0, adjust=False).mean()\n",
        "    data['EMA_26'] = data['Close'].ewm(span=26, min_periods=0, adjust=False).mean()\n",
        "    data['MACD'] = data['EMA_12'] - data['EMA_26']\n",
        "\n",
        "    # Calculate MACD Signal line (9-day EMA of MACD)\n",
        "    data['MACD_Signal'] = data['MACD'].ewm(span=9, min_periods=0, adjust=False).mean()\n",
        "\n",
        "    # Calculate MACD Histogram (MACD - Signal)\n",
        "    data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to interpret MACD signals\n",
        "def interpret_macd(data):\n",
        "    # Buy Signal: MACD crosses its trigger from bottom to top\n",
        "    data['Buy_Signal'] = np.where((data['MACD'] > data['MACD_Signal']) & (data['MACD'].shift(1) < data['MACD_Signal'].shift(1)), \"Buy\", \"\")\n",
        "\n",
        "    # Sell Signal: MACD crosses its trigger from top to bottom\n",
        "    data['Sell_Signal'] = np.where((data['MACD'] < data['MACD_Signal']) & (data['MACD'].shift(1) > data['MACD_Signal'].shift(1)), \"Sell\", \"\")\n",
        "\n",
        "    # Signal Intensity: Average distance of MACD and Trigger line to the midpoint line\n",
        "    data['Signal_Intensity'] = (data['MACD'] + data['MACD_Signal']) / 2\n",
        "\n",
        "    # Upward Momentum: MACD is higher than its nine-day average\n",
        "    data['Upward_Momentum'] = np.where(data['MACD'] > data['MACD'].rolling(window=9).mean(), \"Upward Momentum\", \"\")\n",
        "\n",
        "    # Downward Momentum: MACD is lower than its nine-day average\n",
        "    data['Downward_Momentum'] = np.where(data['MACD'] < data['MACD'].rolling(window=9).mean(), \"Downward Momentum\", \"\")\n",
        "\n",
        "    # Open Bullish Position: MACD shows an uptrend over a period of 14 days\n",
        "    data['Open_Bullish_Position'] = np.where(data['MACD'] > data['MACD'].shift(14), \"Open Bullish Position\", \"\")\n",
        "\n",
        "    # Open Bearish Position: MACD shows a downtrend over a period of 14 days\n",
        "    data['Open_Bearish_Position'] = np.where(data['MACD'] < data['MACD'].shift(14), \"Open Bearish Position\", \"\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Remove existing files in the output path\n",
        "for file_name in os.listdir(macd_output_path):\n",
        "    file_path = os.path.join(macd_output_path, file_name)\n",
        "    os.remove(file_path)\n",
        "\n",
        "# Read data from CSV files in the raw data path\n",
        "for file_name in os.listdir(raw_data_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(raw_data_path, file_name)\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Calculate MACD parameters\n",
        "        df = calculate_macd(df)\n",
        "        # Interpret MACD signals\n",
        "        df = interpret_macd(df)\n",
        "        # Save interpreted MACD data to CSV file in the output path\n",
        "        output_file_path = os.path.join(macd_output_path, file_name)\n",
        "        df.to_csv(output_file_path, index=False, decimal=',')\n",
        "\n",
        "print(\"MACD calculation and interpretation completed. Results saved to the MACD output path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MACD Data Consolidation**"
      ],
      "metadata": {
        "id": "k7rR3RrIOcVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN11qsf8por7"
      },
      "outputs": [],
      "source": [
        "# 07SUMMACD\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "macd_input_path = '/content/drive/MyDrive/DATA/MACD'\n",
        "output_file_path = '/content/drive/MyDrive/DATA/results_macd.csv'\n",
        "\n",
        "# Function to read CSV files and extract rows with the latest date\n",
        "def extract_latest_data(file_path, file_name):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        latest_date = df['Date'].max()  # Find the latest date in the dataframe\n",
        "        latest_data = df[df['Date'] == latest_date].copy()  # Extract rows with the latest date and create a copy\n",
        "        latest_data['File'] = file_name  # Add a new column with the file name\n",
        "        return latest_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate through CSV files in the MACD input path\n",
        "for file_name in os.listdir(macd_input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(macd_input_path, file_name)\n",
        "        # Extract rows with the latest date from the current file\n",
        "        latest_data = extract_latest_data(file_path, file_name)\n",
        "        if latest_data is not None:\n",
        "            # Append the latest data to the list of dataframes\n",
        "            dfs.append(latest_data)\n",
        "\n",
        "# Check if any dataframes were extracted\n",
        "if dfs:\n",
        "    # Concatenate dataframes in the list into a single dataframe\n",
        "    consolidated_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Check if the output file already exists and remove it\n",
        "    if os.path.exists(output_file_path):\n",
        "        os.remove(output_file_path)\n",
        "\n",
        "    # Save the consolidated data to a CSV file\n",
        "    consolidated_data.to_csv(output_file_path, index=False, decimal=',')\n",
        "    print(\"Latest MACD data with file names has been consolidated and saved to:\", output_file_path)\n",
        "else:\n",
        "    print(\"No MACD data found in the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Moving Averages\n",
        "**Usage**\n",
        "- Dependencies: Ensure you have the required Python libraries installed, including pandas.\n",
        "\n",
        "- File Paths: Define the paths for the raw data directory and the output directory for the simple moving average (SMA) analysis. Adjust the raw_data_path and output_path variables to match your directory structure.\n",
        "\n",
        "- Data Preparation: Place your raw data CSV files in the specified raw data directory. Each file should contain a 'Close' column for the calculation of SMAs.\n",
        "\n",
        "- Running the Script: Execute the script to analyze each CSV file in the raw data directory using SMA indicators and save the results to the output directory.\n",
        "\n",
        "**Script Explanation**\n",
        "The script performs the following tasks:\n",
        "\n",
        "- File Paths: Specifies the paths for the raw data directory and the directory where the SMA analysis results will be saved.\n",
        "\n",
        "- Simple Moving Average (SMA) Calculation: Defines a function to calculate SMAs with different window lengths (e.g., 20-day, 50-day, 100-day, 200-day) for each CSV file:\n",
        "\n",
        "- Calculates SMAs using the rolling mean function on the 'Close' price.\n",
        "Interpretation of SMA Signals: Defines a function to interpret SMA signals based on two common technical analysis patterns:\n",
        "\n",
        "1. *Golden Cross: When the shorter-term SMA crosses above the longer-term SMA, suggesting a potential buy signal.*\n",
        "2. *Death Cross: When the shorter-term SMA crosses below the longer-term SMA, suggesting a potential sell signal.*\n",
        "3. *Determines the interpretation based on changes in SMA values over time.*\n",
        "- Processing Each CSV File: Iterates over all CSV files in the raw data directory:\n",
        "\n",
        "1. *Reads each CSV file into a DataFrame.*\n",
        "2. *Calculates multiple SMAs (e.g., 20-day vs 50-day, 100-day vs 200-day).*\n",
        "3. *Interprets SMA signals using the defined functions.*\n",
        "4. *Saves the processed DataFrame with SMA indicators and interpretation signals to a new CSV file in the output directory.*\n",
        "\n",
        "- Execution: After processing all CSV files, the script prints a completion message indicating that the SMA analysis with modified interpretation is finished, and the results are saved in the specified output path."
      ],
      "metadata": {
        "id": "M9YPxwDlOmvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7q__pmcuDiu"
      },
      "outputs": [],
      "source": [
        "# 08MOVAVG\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# Define paths\n",
        "raw_data_path = '/content/drive/MyDrive/DATA/RAW'\n",
        "output_path = '/content/drive/MyDrive/DATA/AVG'\n",
        "\n",
        "# Function to calculate simple moving averages\n",
        "def calculate_sma(data, short_window, long_window):\n",
        "    short_ma = data['Close'].rolling(window=short_window).mean()\n",
        "    long_ma = data['Close'].rolling(window=long_window).mean()\n",
        "    return short_ma, long_ma\n",
        "\n",
        "# Function to interpret SMA signals based on Golden Cross and Death Cross\n",
        "def interpret_sma(short_ma, long_ma):\n",
        "    interpretation = []\n",
        "    for i in range(len(short_ma)):\n",
        "        if short_ma[i] > long_ma[i] and short_ma[i - 1] <= long_ma[i - 1] and short_ma[i] > short_ma[i - 1] and long_ma[i] > long_ma[i - 1]:\n",
        "            interpretation.append(\"Buy Signal - Golden Cross\")\n",
        "        elif short_ma[i] < long_ma[i] and short_ma[i - 1] >= long_ma[i - 1] and short_ma[i] < short_ma[i - 1] and long_ma[i] < long_ma[i - 1]:\n",
        "            interpretation.append(\"Sell Signal - Death Cross\")\n",
        "        else:\n",
        "            interpretation.append(\"No Signal\")\n",
        "    return interpretation\n",
        "\n",
        "# Function to process each CSV file\n",
        "def process_file(file_path):\n",
        "    try:\n",
        "        # Read CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Calculate SMAs\n",
        "        df['SMA_20'], df['SMA_50'] = calculate_sma(df, 20, 50)\n",
        "        df['SMA_100'], df['SMA_200'] = calculate_sma(df, 100, 200)\n",
        "        # Interpret SMA signals\n",
        "        df['20_50_Signal'] = interpret_sma(df['SMA_20'], df['SMA_50'])\n",
        "        df['100_200_Signal'] = interpret_sma(df['SMA_100'], df['SMA_200'])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process each CSV file in the raw data path\n",
        "for file_name in os.listdir(raw_data_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(raw_data_path, file_name)\n",
        "        # Process file\n",
        "        processed_df = process_file(file_path)\n",
        "        if processed_df is not None:\n",
        "            # Save processed data to CSV file in the output path\n",
        "            output_file_path = os.path.join(output_path, file_name)\n",
        "            processed_df.to_csv(output_file_path, index=False, decimal=',')\n",
        "\n",
        "print(\"Simple Moving Average (SMA) analysis with modified interpretation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Moving Averge Data Consolidation**"
      ],
      "metadata": {
        "id": "cLNItrYYPHuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVw1tkIrxdmA"
      },
      "outputs": [],
      "source": [
        "# 09SUMMOVAVG\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "avg_input_path = '/content/drive/MyDrive/DATA/AVG'\n",
        "output_file_path = '/content/drive/MyDrive/DATA/results_avg.csv'\n",
        "\n",
        "# Function to read CSV files and extract rows with the latest date\n",
        "def extract_latest_data(file_path, file_name):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        latest_date = df['Date'].max()  # Find the latest date in the dataframe\n",
        "        latest_data = df[df['Date'] == latest_date].copy()  # Extract rows with the latest date and create a copy\n",
        "        latest_data['File'] = file_name  # Add a new column with the file name\n",
        "        return latest_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate through CSV files in the AVG input path\n",
        "for file_name in os.listdir(avg_input_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(avg_input_path, file_name)\n",
        "        # Extract rows with the latest date from the current file\n",
        "        latest_data = extract_latest_data(file_path, file_name)\n",
        "        if latest_data is not None:\n",
        "            # Append the latest data to the list of dataframes\n",
        "            dfs.append(latest_data)\n",
        "\n",
        "# Check if any dataframes were extracted\n",
        "if dfs:\n",
        "    # Concatenate dataframes in the list into a single dataframe\n",
        "    consolidated_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Check if the output file already exists and remove it\n",
        "    if os.path.exists(output_file_path):\n",
        "        os.remove(output_file_path)\n",
        "\n",
        "    # Save the consolidated data to a CSV file\n",
        "    consolidated_data.to_csv(output_file_path, index=False, decimal=',')\n",
        "    print(\"Latest average data with file names has been consolidated and saved to:\", output_file_path)\n",
        "else:\n",
        "    print(\"No average data found in the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Consolidation\n",
        "**Usage**\n",
        "- Dependencies: Ensure you have the pandas library installed to run this script.\n",
        "\n",
        "- Data Files: Prepare the input data files (results_stochastic.csv, results_rsi.csv, results_macd.csv, results_avg.csv) in the specified data directory (/content/drive/MyDrive/DATA/). These files should contain the results from individual technical analysis scripts.\n",
        "\n",
        "- Output File: Define the output_file_path where the merged summary data will be saved as a CSV file.\n",
        "\n",
        "- Running the Script: Execute the script to merge data from multiple CSV files based on a common column ('File') and save the summarized results to the specified output file.\n",
        "\n",
        "**Script Explanation**\n",
        "The script performs the following tasks:\n",
        "\n",
        "- File Paths: Specifies the paths for the data directory containing individual technical analysis results (data_path) and the path to save the merged summary data (output_file_path).\n",
        "\n",
        "- Reading Data: Reads each of the four CSV files (results_stochastic.csv, results_rsi.csv, results_macd.csv, results_avg.csv) into separate DataFrames (stochastic_data, rsi_data, macd_data, avg_data).\n",
        "\n",
        "- Merging Dataframes: Combines the DataFrames based on a common column ('File'):\n",
        "\n",
        "1. *First merges stochastic_data with rsi_data to include columns related to RSI indicators.*\n",
        "2. *Then merges the resulting DataFrame with macd_data to incorporate MACD indicators.*\n",
        "3. *Finally, merges with avg_data to add SMA signals.*\n",
        "\n",
        "- Column Order: Adjusts the order of columns in the merged DataFrame, moving the 'File' column to the first position for clarity.\n",
        "\n",
        "- Saving Data: Saves the merged DataFrame (merged_data) to a CSV file (output_file_path) without including the index and using ',' as the decimal separator.\n",
        "\n",
        "- Completion Message: Prints a message confirming that the summary data has been saved to the specified output file.\n",
        "\n",
        "This script enables consolidation and comparison of technical analysis results from multiple indicators into a single summary file for further analysis or reporting purposes. Adjustments can be made as needed to handle different sets of data or additional analysis criteria."
      ],
      "metadata": {
        "id": "RuTsrHyyPO2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06a9Z0h9CwPo"
      },
      "outputs": [],
      "source": [
        "# 10MAIN\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "data_path = '/content/drive/MyDrive/DATA/'\n",
        "output_file_path = '/content/drive/MyDrive/DATA/results_main.csv'\n",
        "\n",
        "# Read data from each file\n",
        "stochastic_data = pd.read_csv(data_path + 'results_stochastic.csv')\n",
        "rsi_data = pd.read_csv(data_path + 'results_rsi.csv')\n",
        "macd_data = pd.read_csv(data_path + 'results_macd.csv')\n",
        "avg_data = pd.read_csv(data_path + 'results_avg.csv')\n",
        "\n",
        "# Merge dataframes on the 'File' column\n",
        "merged_data = stochastic_data.merge(rsi_data[['File', 'RSI_14', 'RSI_30', 'RSI_14_Class', 'RSI_30_Class', 'Trend_14', 'Trend_30', 'Bullish_Divergence', 'Bearish_Divergence']], on='File', how='left')\n",
        "merged_data = merged_data.merge(macd_data[['File', 'Buy_Signal', 'Sell_Signal', 'Signal_Intensity', 'Upward_Momentum', 'Downward_Momentum', 'Open_Bullish_Position', 'Open_Bearish_Position']], on='File', how='left')\n",
        "merged_data = merged_data.merge(avg_data[['File', '20_50_Signal', '100_200_Signal']], on='File', how='left')\n",
        "\n",
        "# Move the 'File' column to the first position\n",
        "merged_data = merged_data[['File'] + [col for col in merged_data.columns if col != 'File']]\n",
        "\n",
        "# Save the merged data to a CSV file\n",
        "merged_data.to_csv(output_file_path, index=False, decimal=',')\n",
        "\n",
        "print(\"Summary data saved to:\", output_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNeWHwb0fuHAZuVpTv4Hw/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}